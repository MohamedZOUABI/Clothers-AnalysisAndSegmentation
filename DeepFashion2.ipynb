{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9184f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectron2 not installed. Please install it for full functionality.\n",
      "Setting up mock segmentation for demonstration...\n",
      "Error processing image: OpenCV(4.12.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
      "\n",
      "Make sure the image path is correct and the image exists\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Install required packages:\n",
    "# pip install torch torchvision opencv-python pillow matplotlib detectron2\n",
    "\n",
    "class DeepFashion2Segmentor:\n",
    "    def __init__(self, model_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize DeepFashion2 segmentation model\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to pre-trained DeepFashion2 model\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = None\n",
    "        self.categories = self._get_fashion_categories()\n",
    "        \n",
    "        # Load model if path provided\n",
    "        if model_path:\n",
    "            self.load_model(model_path)\n",
    "    \n",
    "    def _get_fashion_categories(self) -> Dict[int, str]:\n",
    "        \"\"\"Define DeepFashion2 clothing categories\"\"\"\n",
    "        return {\n",
    "            1: 'short_sleeve_top',\n",
    "            2: 'long_sleeve_top', \n",
    "            3: 'short_sleeve_outwear',\n",
    "            4: 'long_sleeve_outwear',\n",
    "            5: 'vest',\n",
    "            6: 'sling',\n",
    "            7: 'shorts',\n",
    "            8: 'trousers',\n",
    "            9: 'skirt',\n",
    "            10: 'short_sleeve_dress',\n",
    "            11: 'long_sleeve_dress',\n",
    "            12: 'vest_dress',\n",
    "            13: 'sling_dress'\n",
    "        }\n",
    "    \n",
    "    def setup_detectron2_model(self):\n",
    "        \"\"\"\n",
    "        Setup DeepFashion2 using Detectron2 framework\n",
    "        Note: You'll need to download the pre-trained weights\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from detectron2.engine import DefaultPredictor\n",
    "            from detectron2.config import get_cfg\n",
    "            from detectron2.model_zoo import model_zoo\n",
    "            \n",
    "            cfg = get_cfg()\n",
    "            # Use a base model and adapt for fashion\n",
    "            cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "            \n",
    "            # Update for DeepFashion2 - you'll need to download the actual weights\n",
    "            cfg.MODEL.WEIGHTS = \"path_to_deepfashion2_weights.pth\"  # Update this path\n",
    "            cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13  # DeepFashion2 has 13 clothing categories\n",
    "            cfg.MODEL.DEVICE = str(self.device)\n",
    "            \n",
    "            self.model = DefaultPredictor(cfg)\n",
    "            return True\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Detectron2 not installed. Please install it for full functionality.\")\n",
    "            return False\n",
    "    \n",
    "    def load_model(self, model_path: str):\n",
    "        \"\"\"Load pre-trained model\"\"\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Model file not found: {model_path}\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.model = torch.load(model_path, map_location=self.device)\n",
    "            self.model.eval()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_image(self, image_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess image for model input\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed image array\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize while maintaining aspect ratio\n",
    "        height, width = image.shape[:2]\n",
    "        max_size = 800\n",
    "        \n",
    "        if max(height, width) > max_size:\n",
    "            scale = max_size / max(height, width)\n",
    "            new_width = int(width * scale)\n",
    "            new_height = int(height * scale)\n",
    "            image = cv2.resize(image, (new_width, new_height))\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def segment_outfit(self, image_path: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Segment clothing items in the outfit\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing detected items and their masks\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            print(\"Model not loaded. Please load a model first.\")\n",
    "            return {}\n",
    "        \n",
    "        # Preprocess image\n",
    "        image = self.preprocess_image(image_path)\n",
    "        \n",
    "        # For demonstration, we'll simulate the segmentation\n",
    "        # In a real implementation, you'd run inference here\n",
    "        results = self._mock_segmentation(image)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _mock_segmentation(self, image: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Mock segmentation for demonstration purposes\n",
    "        Replace this with actual model inference\n",
    "        \"\"\"\n",
    "        height, width = image.shape[:2]\n",
    "        \n",
    "        # Simulate detection results\n",
    "        mock_results = {\n",
    "            'detections': [\n",
    "                {\n",
    "                    'category_id': 1,  # short_sleeve_top\n",
    "                    'category_name': 'short_sleeve_top',\n",
    "                    'bbox': [width//4, height//8, width//2, height//3],  # [x, y, w, h]\n",
    "                    'score': 0.95,\n",
    "                    'mask': np.zeros((height, width), dtype=np.uint8)  # Placeholder mask\n",
    "                },\n",
    "                {\n",
    "                    'category_id': 8,  # trousers\n",
    "                    'category_name': 'trousers', \n",
    "                    'bbox': [width//3, height//2, width//3, height//2],\n",
    "                    'score': 0.92,\n",
    "                    'mask': np.zeros((height, width), dtype=np.uint8)\n",
    "                }\n",
    "            ],\n",
    "            'image_shape': (height, width, 3)\n",
    "        }\n",
    "        \n",
    "        return mock_results\n",
    "    \n",
    "    def extract_clothing_items(self, image_path: str, output_dir: str = \"extracted_items\") -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract individual clothing items from outfit image\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input outfit image\n",
    "            output_dir: Directory to save extracted items\n",
    "            \n",
    "        Returns:\n",
    "            List of paths to extracted item images\n",
    "        \"\"\"\n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(image_path)\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Get segmentation results\n",
    "        results = self.segment_outfit(image_path)\n",
    "        \n",
    "        extracted_items = []\n",
    "        \n",
    "        if 'detections' in results:\n",
    "            for i, detection in enumerate(results['detections']):\n",
    "                # Extract bounding box coordinates\n",
    "                x, y, w, h = detection['bbox']\n",
    "                \n",
    "                # Crop item from original image\n",
    "                item_image = original_image[y:y+h, x:x+w]\n",
    "                \n",
    "                # Save extracted item\n",
    "                item_name = f\"{detection['category_name']}_{i+1}.jpg\"\n",
    "                item_path = os.path.join(output_dir, item_name)\n",
    "                \n",
    "                item_pil = Image.fromarray(item_image)\n",
    "                item_pil.save(item_path)\n",
    "                \n",
    "                extracted_items.append(item_path)\n",
    "                \n",
    "                print(f\"Extracted {detection['category_name']} with confidence {detection['score']:.2f}\")\n",
    "        \n",
    "        return extracted_items\n",
    "    \n",
    "    def visualize_segmentation(self, image_path: str, save_path: str = None):\n",
    "        \"\"\"\n",
    "        Visualize segmentation results\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to input image\n",
    "            save_path: Path to save visualization (optional)\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = self.preprocess_image(image_path)\n",
    "        results = self.segment_outfit(image_path)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        if 'detections' in results:\n",
    "            for detection in results['detections']:\n",
    "                x, y, w, h = detection['bbox']\n",
    "                \n",
    "                # Draw bounding box\n",
    "                rect = plt.Rectangle((x, y), w, h, fill=False, color='red', linewidth=2)\n",
    "                plt.gca().add_patch(rect)\n",
    "                \n",
    "                # Add label\n",
    "                label = f\"{detection['category_name']}\\n{detection['score']:.2f}\"\n",
    "                plt.text(x, y-10, label, color='red', fontsize=10, \n",
    "                        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.title(\"DeepFashion2 Outfit Segmentation\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def download_deepfashion2_model():\n",
    "    \"\"\"\n",
    "    Helper function to download DeepFashion2 model\n",
    "    Note: You'll need to implement the actual download logic\n",
    "    \"\"\"\n",
    "    print(\"Please download the DeepFashion2 model from:\")\n",
    "    print(\"https://github.com/switchablenorms/DeepFashion2\")\n",
    "    print(\"Or use the official model zoo links provided in their repository\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize segmentor\n",
    "    segmentor = DeepFashion2Segmentor()\n",
    "    \n",
    "    # Setup model (you'll need to download weights first)\n",
    "    model_loaded = segmentor.setup_detectron2_model()\n",
    "    \n",
    "    if not model_loaded:\n",
    "        print(\"Setting up mock segmentation for demonstration...\")\n",
    "    \n",
    "    # Example image path (replace with your image)\n",
    "    image_path = \"path_to_your_outfit_image.jpg\"\n",
    "    \n",
    "    # Extract clothing items\n",
    "    try:\n",
    "        extracted_items = segmentor.extract_clothing_items(image_path)\n",
    "        print(f\"Extracted {len(extracted_items)} clothing items:\")\n",
    "        for item in extracted_items:\n",
    "            print(f\"  - {item}\")\n",
    "        \n",
    "        # Visualize results\n",
    "        segmentor.visualize_segmentation(image_path, \"segmentation_result.jpg\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        print(\"Make sure the image path is correct and the image exists\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef516a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "segmentor = DeepFashion2Segmentor()\n",
    "segmentor.setup_detectron2_model()\n",
    "\n",
    "# Process outfit image\n",
    "items = segmentor.extract_clothing_items(\"my_outfit.jpg\")\n",
    "segmentor.visualize_segmentation(\"my_outfit.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716737c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd6197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
